{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6QpCUiB5m-8",
        "outputId": "fe6844e3-060d-4271-94e4-3efe3dcdd864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nilearn\n",
            "  Downloading nilearn-0.9.2-py3-none-any.whl (9.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.6 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.8/dist-packages (from nilearn) (1.21.6)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.8/dist-packages (from nilearn) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from nilearn) (1.7.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.8/dist-packages (from nilearn) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.8/dist-packages (from nilearn) (1.0.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from nilearn) (4.9.1)\n",
            "Requirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.8/dist-packages (from nilearn) (1.2.0)\n",
            "Requirement already satisfied: nibabel>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from nilearn) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0->nilearn) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->nilearn) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2->nilearn) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2->nilearn) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2->nilearn) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2->nilearn) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->nilearn) (3.1.0)\n",
            "Installing collected packages: nilearn\n",
            "Successfully installed nilearn-0.9.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/nilearn/input_data/__init__.py:27: FutureWarning: The import path 'nilearn.input_data' is deprecated in version 0.9. Importing from 'nilearn.input_data' will be possible at least until release 0.13.0. Please import from 'nilearn.maskers' instead.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "get_ipython().system(u'pip install nilearn')\n",
        "\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "\n",
        "from nilearn import datasets\n",
        "from nilearn.input_data import NiftiMasker\n",
        "\n",
        "from nilearn.image import smooth_img\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import keras\n",
        "import tensorflow.compat.v1 as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.disable_v2_behavior()\n",
        "\n",
        "n_subjects = 416\n",
        "\n",
        "oasis_dataset = datasets.fetch_oasis_vbm(n_subjects=n_subjects)\n",
        "gray_matter_map_filenames = oasis_dataset.gray_matter_maps\n",
        "gm_imgs = gray_matter_map_filenames"
      ],
      "metadata": {
        "id": "9QAS_0mc5rpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdr = oasis_dataset.ext_vars['cdr'].astype(float)\n",
        "cdr_numpy_arr = np.array(cdr)\n",
        "for i in range(len(cdr_numpy_arr)):\n",
        "    if(np.isnan(cdr_numpy_arr[i])): cdr_numpy_arr[i] = 1\n",
        "    \n",
        "    elif(cdr_numpy_arr[i] > 0.0): cdr_numpy_arr[i] = 1"
      ],
      "metadata": {
        "id": "aCtdWaYo5zF9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgArr = []\n",
        "\n",
        "for imgUrl in gray_matter_map_filenames:\n",
        "    result_img = smooth_img(imgUrl, fwhm=1)\n",
        "    imgArr.append(result_img.get_data())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKWcbX2l52zx",
        "outputId": "3a6d1899-4ddc-4550-8760-c621bf5f7b9c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-a233924848ec>:5: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
            "\n",
            "* deprecated from version: 3.0\n",
            "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
            "  imgArr.append(result_img.get_data())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "x_test = []\n",
        "\n",
        "y_train = []\n",
        "y_test = []\n",
        "\n",
        "rshapedImgArr = []\n",
        "\n",
        "for img in imgArr:\n",
        "    newImg = [cv2.resize(each_slice,(50,50)) for each_slice in img]#Reducing slice count\n",
        "    newImg = np.array(newImg)\n",
        "    rshapedImgArr.append(newImg)\n",
        "    \n",
        "label = cdr_numpy_arr"
      ],
      "metadata": {
        "id": "PHrmUqUm564z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = keras.utils.to_categorical(cdr_numpy_arr, 2)\n",
        "\n",
        "much_data = []\n",
        "\n",
        "for num, img in enumerate(rshapedImgArr):\n",
        "    much_data.append([img,label[num]])"
      ],
      "metadata": {
        "id": "wt3TD_CZ59Xq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE_PX_X = 50\n",
        "IMG_SIZE_PX_Y = 50\n",
        "SLICE_COUNT = 91\n",
        "\n",
        "n_classes = 2\n",
        "batch_size = 10\n",
        "\n",
        "x = tf.placeholder('float')\n",
        "y = tf.placeholder('float')\n",
        "\n",
        "keep_rate = 0.8"
      ],
      "metadata": {
        "id": "xEf6_NhO5_qp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3d(x, W):\n",
        "    conv = tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
        "    conv = tf.nn.dropout(conv, 0.5)\n",
        "    return conv\n",
        "\n",
        "def maxpool3d(x):\n",
        "    #                        size of window         movement of window as you slide about\n",
        "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
        "\n"
      ],
      "metadata": {
        "id": "rnRIZGA46DPC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convolutional_neural_network(x):\n",
        "    #                # 5 x 5 x 5 patches, 1 channel, 32 features to compute.\n",
        "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n",
        "               #       5 x 5 x 5 patches, 32 channels, 64 features to compute.\n",
        "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n",
        "               #                                  64 features\n",
        "               'W_fc':tf.Variable(tf.random_normal([248768,1024])),\n",
        "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
        "\n",
        "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
        "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
        "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
        "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
        "\n",
        "    #                            image X      image Y        image Z\n",
        "    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX_X, IMG_SIZE_PX_Y, SLICE_COUNT, 1])\n",
        "\n",
        "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
        "    conv1 = maxpool3d(conv1)\n",
        "\n",
        "\n",
        "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
        "    conv2 = maxpool3d(conv2)\n",
        "\n",
        "    fc = tf.reshape(conv2,[-1, 248768])\n",
        "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
        "    fc = tf.nn.dropout(fc, keep_rate)\n",
        "\n",
        "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "isNyk2E46GTg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data = much_data[:-333]\n",
        "# validation_data = much_data[-83:]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_neural_network(x):\n",
        "    prediction = convolutional_neural_network(x)\n",
        "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
        "    \n",
        "    #file = open(\"output.txt\", \"w\");\n",
        "    file = open(\"output.txt\", \"w\");\n",
        "    \n",
        "    hm_epochs = 25\n",
        "    #hm_epochs = 2\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "        \n",
        "        successful_runs = 0\n",
        "        total_runs = 0\n",
        "        \n",
        "        for epoch in range(hm_epochs):\n",
        "            epoch_loss = 0\n",
        "            \n",
        "            train_data, validation_data = train_test_split(much_data, train_size=0.8)\n",
        "            \n",
        "            for data in train_data:\n",
        "                total_runs += 1\n",
        "                try:\n",
        "                    X = data[0]\n",
        "                    Y = data[1]\n",
        "                    _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
        "                    epoch_loss += c\n",
        "                    successful_runs += 1\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "                    #print(str(e))\n",
        "            \n",
        "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
        "            #f.write(\"{} {} {}\\n\".format(a, b, c))\n",
        "            file.write('Epoch{}completed out of{}loss:{}\\n'.format(epoch+1, hm_epochs, epoch_loss));\n",
        "            #file.writelines('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss);\n",
        "            #file.write('Epoch')\n",
        "            #file.write(str(epoch+1))\n",
        "            #file.write('completed out of',hm_epochs)\n",
        "            #file.write('completed out of ')\n",
        "            #file.write(str(hm_epochs))\n",
        "            #file.write('loss: ')\n",
        "            #file.write('loss:',epoch_loss);\n",
        "            #file.write(str(epoch_loss))\n",
        "\n",
        "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "\n",
        "            print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
        "            #file.writelines('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
        "            file.write('Accuracy:{}\\n'.format(accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]})))\n",
        "            #file.write('Accuracy: ')\n",
        "            #file.write(str(accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]})))\n",
        "        print('Done. Finishing accuracy:')\n",
        "        print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
        "        \n",
        "        print('fitment percent:',successful_runs/total_runs)\n",
        "        \n",
        "        file.write('Done. Finishing accuracy:')\n",
        "        #file.writelines('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
        "        file.write('Accuracy:{}\\n'.format(accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]})))\n",
        "        #file.write('Accuracy: ')\n",
        "        #file.write(str(accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]})))\n",
        "        \n",
        "        #file.write('fitment percent: ')\n",
        "        #file.write(str(successful_runs/total_runs))\n",
        "        #file.writelines('fitment percent:',successful_runs/total_runs)\n",
        "        file.write('fitment percent{}'.format(successful_runs/total_runs))\n",
        "\n",
        "train_neural_network(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwbehxWi6LuN",
        "outputId": "4bce6872-b96d-468a-c4fa-ae67e12b200f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/tf_should_use.py:243: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed out of 25 loss: 307297691.34375\n",
            "Accuracy: 0.79012346\n",
            "Epoch 2 completed out of 25 loss: 82022028.5703125\n",
            "Accuracy: 0.654321\n",
            "Epoch 3 completed out of 25 loss: 36057431.34375\n",
            "Accuracy: 0.67901236\n",
            "Epoch 4 completed out of 25 loss: 16053137.67578125\n",
            "Accuracy: 0.5802469\n",
            "Epoch 5 completed out of 25 loss: 11933029.373046875\n",
            "Accuracy: 0.6666667\n",
            "Epoch 6 completed out of 25 loss: 8874264.66796875\n",
            "Accuracy: 0.6296296\n",
            "Epoch 7 completed out of 25 loss: 7979462.517578125\n",
            "Accuracy: 0.7407407\n",
            "Epoch 8 completed out of 25 loss: 5792025.097290039\n",
            "Accuracy: 0.654321\n",
            "Epoch 9 completed out of 25 loss: 4797067.879394531\n",
            "Accuracy: 0.56790125\n",
            "Epoch 10 completed out of 25 loss: 4000727.842529297\n",
            "Accuracy: 0.6419753\n",
            "Epoch 11 completed out of 25 loss: 3249982.8828125\n",
            "Accuracy: 0.5925926\n",
            "Epoch 12 completed out of 25 loss: 3066452.123046875\n",
            "Accuracy: 0.67901236\n",
            "Epoch 13 completed out of 25 loss: 2155280.3774294853\n",
            "Accuracy: 0.6296296\n",
            "Epoch 14 completed out of 25 loss: 1741184.709678607\n",
            "Accuracy: 0.67901236\n",
            "Epoch 15 completed out of 25 loss: 1337544.6068725586\n",
            "Accuracy: 0.6419753\n",
            "Epoch 16 completed out of 25 loss: 1418886.1887207031\n",
            "Accuracy: 0.54320985\n",
            "Epoch 17 completed out of 25 loss: 1020575.1828308105\n",
            "Accuracy: 0.8148148\n",
            "Epoch 18 completed out of 25 loss: 802806.6737060547\n",
            "Accuracy: 0.6296296\n",
            "Epoch 19 completed out of 25 loss: 735682.6838350296\n",
            "Accuracy: 0.56790125\n",
            "Epoch 20 completed out of 25 loss: 556312.2408385277\n",
            "Accuracy: 0.72839504\n",
            "Epoch 21 completed out of 25 loss: 497319.62701416016\n",
            "Accuracy: 0.60493827\n",
            "Epoch 22 completed out of 25 loss: 369022.8134765625\n",
            "Accuracy: 0.5185185\n",
            "Epoch 23 completed out of 25 loss: 338689.2507972717\n",
            "Accuracy: 0.6296296\n",
            "Epoch 24 completed out of 25 loss: 217798.04537582397\n",
            "Accuracy: 0.5925926\n",
            "Epoch 25 completed out of 25 loss: 209633.5781711042\n",
            "Accuracy: 0.56790125\n",
            "Done. Finishing accuracy:\n",
            "Accuracy: 0.6296296\n",
            "fitment percent: 1.0\n"
          ]
        }
      ]
    }
  ]
}